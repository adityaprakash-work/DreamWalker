{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DEPENDENCIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local import\n",
    "# import sys\n",
    "# sys.path.append('..')\n",
    "\n",
    "# Import on colab\n",
    "%pip install git+https://github.com/adityaprakash-work/DreamWalker.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "from google.colab import drive\n",
    "import dreamwalker as dw\n",
    "from dreamwalker.pytorch_generative import models, trainer\n",
    "from dreamwalker.models.brain import ConformerEEGEncoder\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load from an online source\n",
    "dataset_url = \"https://www.kaggle.com/datasets/ifigotin/imagenetmini-1000/data\"\n",
    "dataset_dir = \"/content/dataset\"\n",
    "dw.utils.datasets.download(dataset_url, dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"/content/dataset/imagenetmini-1000/imagenet-mini/train\"\n",
    "valid_dir = \"/content/dataset/imagenetmini-1000/imagenet-mini/val\"\n",
    "if valid_dir is None:\n",
    "    dataset = dw.utils.datasets.ImageStream(train_dir, ext=\"JPEG\")\n",
    "    train_loader, valid_loader = dw.utils.datasets.get_loaders(\n",
    "        dataset, return_valid=True, valid_size=0.2\n",
    "    )\n",
    "\n",
    "else:\n",
    "    train_dataset = dw.utils.datasets.ImageStream(train_dir, ext=\"JPEG\")\n",
    "    valid_dataset = dw.utils.datasets.ImageStream(valid_dir, ext=\"JPEG\")\n",
    "    train_loader = dw.utils.datasets.get_loaders(train_dataset, batch_size=16)\n",
    "    valid_loader = dw.utils.datasets.get_loaders(valid_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_data_path = \"/content/drive/MyDrive/Brain2Image/raw_eeg2/\"\n",
    "img_data_path = \"/content/drive/MyDrive/Brain2Image/sqr_images2/\"\n",
    "all_classes = np.unique([i.split(\"_\")[0] for i in os.listdir(img_data_path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_classes = list(np.random.choice(all_classes, 40))\n",
    "use_eegs = [\n",
    "    p for c in use_classes for p in glob.glob(f\"{eeg_data_path}*{c}*\")\n",
    "]\n",
    "random.shuffle(use_eegs)\n",
    "use_imgs = [\n",
    "    os.path.join(img_data_path, \"_\".join(os.path.basename(p).split(\"_\")[2:4]))\n",
    "    for p in use_eegs\n",
    "]\n",
    "use_clsl = [\n",
    "    os.path.basename(p).split(\"_\")[0] for p in use_imgs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DreamWalkerDataset(Dataset):\n",
    "    def __init__(self, eeg_data_path, img_data_path, use_eegs, transform=None):\n",
    "        self.eeg_data_path = eeg_data_path\n",
    "        self.img_data_path = img_data_path\n",
    "        self.use_eegs = use_eegs\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.use_eegs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eeg_path = self.use_eegs[idx]\n",
    "        img_path = os.path.join(\n",
    "            self.img_data_path, \"_\".join(os.path.basename(eeg_path).split(\"_\")[2:4])\n",
    "        )\n",
    "\n",
    "        eeg_data = np.load(eeg_path).T\n",
    "        eeg_data = np.expand_dims(eeg_data, 0)\n",
    "        img_data = np.load(img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            img_data = self.transform(img_data)\n",
    "\n",
    "        return eeg_data, img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = DreamWalkerDataset(\n",
    "    eeg_data_path, img_data_path, use_eegs, transform=transform\n",
    ")\n",
    "\n",
    "lt = int(0.8 * len(dataset))\n",
    "lv = len(dataset) - lt\n",
    "\n",
    "td, vd = random_split(dataset, [lt, lv])\n",
    "\n",
    "dw_train_loader = DataLoader(td, batch_size=16, shuffle=True)\n",
    "dw_valid_loader = DataLoader(vd, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **VQVAE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae_model = models.VectorQuantizedVAE2(\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    hidden_channels=128,\n",
    "    n_residual_blocks=2,\n",
    "    residual_channels=64,\n",
    "    n_embeddings=512,\n",
    "    embedding_dim=64,\n",
    ")\n",
    "\n",
    "vqvae_optimizer = optim.Adam(vqvae_model.parameters(), lr=2e-4)\n",
    "vqvae_scheduler = lr_scheduler.MultiplicativeLR(\n",
    "    vqvae_optimizer, lr_lambda=lambda _: 0.999977\n",
    ")\n",
    "\n",
    "\n",
    "def vqvae_loss_fn(x, _, preds):\n",
    "    preds, vq_loss = preds\n",
    "    recon_loss = F.mse_loss(preds, x)\n",
    "    loss = recon_loss + 0.25 * vq_loss\n",
    "\n",
    "    return {\n",
    "        \"vq_loss\": vq_loss,\n",
    "        \"reconstruction_loss\": recon_loss,\n",
    "        \"loss\": loss,\n",
    "    }\n",
    "\n",
    "vqvae_model_trainer = trainer.Trainer(\n",
    "    model=vqvae_model,\n",
    "    loss_fn=vqvae_loss_fn,\n",
    "    optimizer=vqvae_optimizer,\n",
    "    train_loader=train_loader,\n",
    "    eval_loader=valid_loader,\n",
    "    lr_scheduler=vqvae_scheduler,\n",
    "    log_dir=\"/content/logs/vqvae0\",\n",
    "    n_gpus=1,\n",
    ")\n",
    "\n",
    "vqvae_model_trainer.restore_checkpoint(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ALIGNMENT MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignmentModel(torch.nn.Module):\n",
    "    def __init__(self, brain_module, vqvae2_module, vgrad=False):\n",
    "        super().__init__()\n",
    "        self.brain_module = brain_module\n",
    "        self.vqvae_module = vqvae2_module\n",
    "        if not vgrad:\n",
    "            self.vqvae_module.requires_grad_(False)\n",
    "        \n",
    "\n",
    "    def forward(self, x, y):\n",
    "        zb, zt = self.brain_module(x)\n",
    "\n",
    "        encoded_b = self.vqvae_module._encoder_b(y)\n",
    "        encoded_t = self.vqvae_module._encoder_t(encoded_b)\n",
    "\n",
    "        # Cosine loss on the encoded features\n",
    "        talgn_loss = 1 - F.cosine_similarity(zt, encoded_t).mean()\n",
    "        balgn_loss = 1 - F.cosine_similarity(zb, encoded_b).mean()\n",
    "        algn_loss = talgn_loss + balgn_loss\n",
    "\n",
    "        quantized_t, vq_loss_t = self.vqvae_module._quantizer_t(zt)\n",
    "        quantized_b, vq_loss_b = self.vqvae_module._quantizer_b(zb)\n",
    "\n",
    "        decoded_t = self.vqvae_module._decoder_t(quantized_t)\n",
    "        xhat = self.vqvae_module._decoder_b(\n",
    "            torch.cat((self.vqvae_module._conv(decoded_t), quantized_b), dim=1)\n",
    "        )\n",
    "\n",
    "        vq_loss = 0.5 * (vq_loss_b + vq_loss_t) + F.mse_loss(decoded_t, encoded_b)\n",
    "\n",
    "        return xhat, vq_loss, algn_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algn_model = AlignmentModel(\n",
    "    dw.models.brain.ConformerEEGEncoder(),\n",
    "    vqvae_model,\n",
    ")\n",
    "\n",
    "algn_optimizer = optim.Adam(algn_model.brain_module.parameters(), lr=2e-4)\n",
    "algn_scheduler = lr_scheduler.MultiplicativeLR(\n",
    "    algn_optimizer, lr_lambda=lambda _: 0.999977\n",
    ")\n",
    "\n",
    "\n",
    "def algn_loss_fn(x, y, preds):\n",
    "    xhat, _, algn_loss = preds\n",
    "    recon_loss = F.mse_loss(preds, y)\n",
    "    loss = algn_loss + 0.25 * recon_loss\n",
    "\n",
    "    return {\n",
    "        \"algn_loss\": algn_loss,\n",
    "        \"reconstruction_loss\": recon_loss,\n",
    "        \"loss\": loss,\n",
    "    }\n",
    "\n",
    "\n",
    "agn_model_trainer = trainer.Trainer(\n",
    "    model=algn_model,\n",
    "    loss_fn=algn_loss_fn,\n",
    "    optimizer=algn_optimizer,\n",
    "    train_loader=dw_train_loader,\n",
    "    eval_loader=dw_valid_loader,\n",
    "    lr_scheduler=algn_scheduler,\n",
    "    log_dir=\"/content/logs/algn0\",\n",
    "    n_gpus=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grid_ovsr(original, reconstructions):\n",
    "    num_samples=original.shape[0]\n",
    "    num_rows = int(np.ceil(np.sqrt(num_samples)))\n",
    "    grid_o = make_grid(original, nrow=num_rows, normalize=True)\n",
    "    grid_r = make_grid(reconstructions, nrow=num_rows, normalize=True)\n",
    "    grid = torch.cat([grid_o, grid_r], dim=-1)\n",
    "    return grid\n",
    "\n",
    "def recplt_monitor(model_trainer):\n",
    "    model_trainer.model.eval()\n",
    "    x, y = next(iter(model_trainer.eval_loader))\n",
    "    x = x.to(model_trainer.device)\n",
    "    x_recon, _, _ = model_trainer.model(x)\n",
    "    x_recon = x_recon.cpu().detach()\n",
    "    model_trainer._summary_writer.add_image(\n",
    "        \"Reconstruction Fidelity\",\n",
    "        make_grid_ovsr(y, x_recon),\n",
    "        model_trainer._step,\n",
    "    )\n",
    "    model_trainer.model.train()\n",
    "\n",
    "algn_model.interleaved_train_and_eval(7, arbitrary_monitors=[recplt_monitor])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
